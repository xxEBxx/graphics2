{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# **Tomato Segmentation with U-Net**  \n",
    "\n",
    "This notebook provides an **end-to-end implementation** of tomato image segmentation using a U-Net convolutional neural network. Designed to run on **Google Colab with free GPU acceleration**\n",
    "\n",
    "### ðŸš€ **How to Use This Notebook**  \n",
    "1. **Enable GPU**:  \n",
    "   Go to `Runtime` â†’ `Change runtime type` â†’ Select `GPU`  \n",
    "   *(Verify with `torch.cuda.is_available()` in Cell 1)*  \n",
    "\n",
    "2. **Upload Your Data**:  \n",
    "   - Run Cell 2 to upload `Tomato_dataset.zip`  \n",
    "   - Expected structure:  \n",
    "     ```\n",
    "     Tomato_dataset/  \n",
    "     â”œâ”€â”€ Train/      # Training images  \n",
    "     â”œâ”€â”€ Mask/       # Training masks  \n",
    "     â”œâ”€â”€ Test2/       # Test images  \n",
    "     â””â”€â”€ Mask2/ # Ground truth masks (for evaluation)  \n",
    "     ```  \n",
    "\n",
    "3. **Execute Cells Sequentially**:  \n",
    "   - Cells 3-7: Build and train the U-Net model  \n",
    "   - Cells 8-9: Evaluate on test images and visualize results  \n",
    "   - Cell 10-11: Download predictions  \n",
    "\n",
    "\n",
    "\n",
    "### ðŸ“ˆ **Example Output**  \n",
    "After training, you'll get:  \n",
    "- Trained model (`unet_tomato.pth`)  \n",
    "- Predicted masks for test images  \n",
    "- Performance metrics like:  \n",
    "  ```\n",
    "  IoU: 0.82 | Dice: 0.90  \n",
    "  Precision: 0.88 | Recall: 0.85  \n",
    "  ```\n",
    "\n",
    "ðŸ“‚ **Output Folder Contents**  \n",
    "After processing, `/content/Tomato_dataset/output_masks/` contains:  \n",
    "\n",
    "- `[image_name]_mask.png` â†’ Predicted segmentation mask (binary image)  \n",
    "- `vis_[image_name].jpg` â†’ **Visual comparison** with three panels:  \n",
    "  - **Left**: Original input image  \n",
    "  - **Middle**: Predicted mask (white=tomato, black=background)  \n",
    "  - **Right**: Ground truth mask (if provided in `Test_Masks/`)  \n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4-lZyK0PRStc",
    "outputId": "9bec5248-495a-463d-d26a-119acd8405f5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Should return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QNjzWhaPO5w9"
   },
   "source": [
    "## Step 1/2: Dataset loading and preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "id": "4YoDLZonYC2V",
    "outputId": "58e7073d-ae45-4fb4-c34e-d54bfdc0c110"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-59f2a0c6-39d9-4fee-ba14-fde09bbc17f6\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-59f2a0c6-39d9-4fee-ba14-fde09bbc17f6\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Tomato_dataset.zip to Tomato_dataset.zip\n",
      "['Mask2', 'Mask', 'Test', 'Train2', 'Test2', 'Train']\n"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Upload the zip file (click the upload button or use this code)\n",
    "uploaded = files.upload()  # Select Tomato_dataset.zip\n",
    "\n",
    "# Extract the zip file\n",
    "with zipfile.ZipFile('Tomato_dataset.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('/content/')  # Extracts to /content/Tomato_dataset/\n",
    "\n",
    "# Verify extraction\n",
    "print(os.listdir('/content/Tomato_dataset/'))  # Should show ['Train', 'Mask']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2aGp1bIKPLH-"
   },
   "source": [
    "## Step 3/6: Dataset class with preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "2I6i6surU352"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class TomatoDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # Only include image files (exclude hidden files/folders)\n",
    "        self.images = [\n",
    "            f for f in os.listdir(image_dir)\n",
    "            if f.endswith(('.jpg'))  # Add your image formats\n",
    "            and not f.startswith('.')  # Exclude hidden files\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.images[idx]\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        mask_name = img_name\n",
    "        mask_path = os.path.join(self.mask_dir, mask_name)\n",
    "\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            mask = Image.open(mask_path).convert('L')\n",
    "\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "                mask = transforms.Resize((256, 256), interpolation=Image.NEAREST)(mask)\n",
    "                mask = transforms.ToTensor()(mask)\n",
    "                mask = (mask > 0).float()\n",
    "\n",
    "            return image, mask\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {img_name}: {str(e)}\")\n",
    "            # Return a placeholder or skip this sample\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LqXgV-R0PZD7"
   },
   "source": [
    "## Step 4/5: U-Net implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "aTcHbEwJVQGf"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels=3, n_classes=1):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        # Encoder (Downsampling)\n",
    "        self.down1 = DoubleConv(n_channels, 64)\n",
    "        self.down2 = DoubleConv(64, 128)\n",
    "        self.down3 = DoubleConv(128, 256)\n",
    "        self.down4 = DoubleConv(256, 512)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = DoubleConv(512, 1024)\n",
    "\n",
    "        # Decoder (Upsampling)\n",
    "        self.up1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.up_conv1 = DoubleConv(1024, 512)\n",
    "\n",
    "        self.up2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.up_conv2 = DoubleConv(512, 256)\n",
    "\n",
    "        self.up3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.up_conv3 = DoubleConv(256, 128)\n",
    "\n",
    "        self.up4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.up_conv4 = DoubleConv(128, 64)\n",
    "\n",
    "        # Output layer\n",
    "        self.out = nn.Conv2d(64, n_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x1 = self.down1(x)\n",
    "        x2 = self.pool(x1)\n",
    "        x2 = self.down2(x2)\n",
    "        x3 = self.pool(x2)\n",
    "        x3 = self.down3(x3)\n",
    "        x4 = self.pool(x3)\n",
    "        x4 = self.down4(x4)\n",
    "        x5 = self.pool(x4)\n",
    "\n",
    "        # Bottleneck\n",
    "        x5 = self.bottleneck(x5)\n",
    "\n",
    "        # Decoder with skip connections\n",
    "        x = self.up1(x5)\n",
    "        x = torch.cat([x, x4], dim=1)\n",
    "        x = self.up_conv1(x)\n",
    "\n",
    "        x = self.up2(x)\n",
    "        x = torch.cat([x, x3], dim=1)\n",
    "        x = self.up_conv2(x)\n",
    "\n",
    "        x = self.up3(x)\n",
    "        x = torch.cat([x, x2], dim=1)\n",
    "        x = self.up_conv3(x)\n",
    "\n",
    "        x = self.up4(x)\n",
    "        x = torch.cat([x, x1], dim=1)\n",
    "        x = self.up_conv4(x)\n",
    "\n",
    "        # Output\n",
    "        return torch.sigmoid(self.out(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lrBnYwQPPdcn"
   },
   "source": [
    "## Step 12: Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "IEJdmNQfEy_e"
   },
   "outputs": [],
   "source": [
    "# ==================== EVALUATION METRICS ====================\n",
    "def calculate_metrics(pred, target, threshold=0.5):\n",
    "    \"\"\"Computes IoU, Dice, Accuracy, Precision, Recall\"\"\"\n",
    "    pred_bin = (pred > threshold).float()\n",
    "    target_bin = target.float()\n",
    "\n",
    "    tp = torch.sum(pred_bin * target_bin).item()\n",
    "    fp = torch.sum(pred_bin * (1 - target_bin)).item()\n",
    "    fn = torch.sum((1 - pred_bin) * target_bin).item()\n",
    "    tn = torch.sum((1 - pred_bin) * (1 - target_bin)).item()\n",
    "\n",
    "    eps = 1e-6\n",
    "    return {\n",
    "        'iou': tp / (tp + fp + fn + eps),\n",
    "        'dice': (2 * tp) / (2 * tp + fp + fn + eps),\n",
    "        'accuracy': (tp + tn) / (tp + tn + fp + fn + eps),\n",
    "        'precision': tp / (tp + fp + eps),\n",
    "        'recall': tp / (tp + fn + eps)\n",
    "    }\n",
    "\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    \"\"\"Evaluates model on a DataLoader\"\"\"\n",
    "    model.eval()\n",
    "    metrics = {'iou':0, 'dice':0, 'accuracy':0, 'precision':0, 'recall':0}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks in dataloader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            outputs = model(images)\n",
    "            batch_metrics = calculate_metrics(outputs, masks)\n",
    "\n",
    "            for key in metrics:\n",
    "                metrics[key] += batch_metrics[key]\n",
    "\n",
    "    for key in metrics:\n",
    "        metrics[key] /= len(dataloader)\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9E3xavLcPk6m"
   },
   "source": [
    "## Step 7/8/9: Training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "wtMyhIIvVS15"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# Step 6: Preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Step 2-3: Dataset preparation\n",
    "# Use absolute paths in Colab\n",
    "dataset = TomatoDataset(\n",
    "    image_dir='/content/Tomato_dataset/Train',  # Full path\n",
    "    mask_dir='/content/Tomato_dataset/Mask',    # Full path\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Split into train and validation\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "# Step 5: Model implementation\n",
    "# Initialize model, loss, and optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = UNet().to(device)\n",
    "criterion = nn.BCELoss()   # Step 8: Loss function : Binary Cross Entropy Loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001) # Step 9: Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EGq9DtmyQTqR"
   },
   "source": [
    "## Step 7/10: Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JxmidsMuVVHQ",
    "outputId": "63453032-947d-4661-e837-91a92d0a2098"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.3615\n",
      "Validation Loss: 0.2918\n",
      "IoU: 0.7463 | Dice: 0.8480 | Acc: 0.9594\n",
      "Precision: 0.8775 | Recall: 0.8347\n",
      "Epoch 2, Loss: 0.2444\n",
      "Validation Loss: 0.2101\n",
      "IoU: 0.8038 | Dice: 0.8828 | Acc: 0.9641\n",
      "Precision: 0.8392 | Recall: 0.9526\n",
      "Epoch 3, Loss: 0.2012\n",
      "Validation Loss: 0.1756\n",
      "IoU: 0.7951 | Dice: 0.8767 | Acc: 0.9610\n",
      "Precision: 0.8179 | Recall: 0.9677\n",
      "Epoch 4, Loss: 0.1792\n",
      "Validation Loss: 0.1488\n",
      "IoU: 0.8002 | Dice: 0.8796 | Acc: 0.9626\n",
      "Precision: 0.8321 | Recall: 0.9557\n",
      "Epoch 5, Loss: 0.1546\n",
      "Validation Loss: 0.1359\n",
      "IoU: 0.8076 | Dice: 0.8856 | Acc: 0.9658\n",
      "Precision: 0.8514 | Recall: 0.9432\n",
      "Epoch 6, Loss: 0.1460\n",
      "Validation Loss: 0.1339\n",
      "IoU: 0.7829 | Dice: 0.8682 | Acc: 0.9576\n",
      "Precision: 0.8088 | Recall: 0.9613\n",
      "Epoch 7, Loss: 0.1322\n",
      "Validation Loss: 0.1084\n",
      "IoU: 0.8086 | Dice: 0.8861 | Acc: 0.9656\n",
      "Precision: 0.8416 | Recall: 0.9560\n",
      "Epoch 8, Loss: 0.1305\n",
      "Validation Loss: 0.1233\n",
      "IoU: 0.7958 | Dice: 0.8769 | Acc: 0.9612\n",
      "Precision: 0.8186 | Recall: 0.9670\n",
      "Epoch 9, Loss: 0.1220\n",
      "Validation Loss: 0.1045\n",
      "IoU: 0.8157 | Dice: 0.8913 | Acc: 0.9681\n",
      "Precision: 0.8558 | Recall: 0.9477\n",
      "Epoch 10, Loss: 0.1170\n",
      "Validation Loss: 0.0935\n",
      "IoU: 0.8117 | Dice: 0.8882 | Acc: 0.9665\n",
      "Precision: 0.8442 | Recall: 0.9562\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, masks in train_loader:\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader):.4f}')\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_metrics = {'iou':0, 'dice':0, 'accuracy':0, 'precision':0, 'recall':0}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks in val_loader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            outputs = model(images)\n",
    "\n",
    "            val_loss += criterion(outputs, masks).item()\n",
    "            batch_metrics = calculate_metrics(outputs, masks)\n",
    "\n",
    "            for key in val_metrics:\n",
    "                val_metrics[key] += batch_metrics[key]\n",
    "\n",
    "    # Average metrics\n",
    "    for key in val_metrics:\n",
    "        val_metrics[key] /= len(val_loader)\n",
    "\n",
    "    print(f'Validation Loss: {val_loss/len(val_loader):.4f}')\n",
    "    print(f'IoU: {val_metrics[\"iou\"]:.4f} | Dice: {val_metrics[\"dice\"]:.4f} | Acc: {val_metrics[\"accuracy\"]:.4f}')\n",
    "    print(f'Precision: {val_metrics[\"precision\"]:.4f} | Recall: {val_metrics[\"recall\"]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "is17-tvqZvZD"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '/content/Tomato_dataset/unet_tomato.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PFbNRBwvVXdX",
    "outputId": "2f2b1930-0bbe-4be6-8253-f41942d6cec4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Img10.jpg\n",
      "Processed Img6.jpg\n",
      "Processed Img20.jpg\n",
      "Processed Img12.jpg\n",
      "Processed Img11.jpg\n",
      "Processed Img18.jpg\n",
      "Processed Img13.jpg\n",
      "Processed Img3.jpg\n",
      "Processed Img15.jpg\n",
      "Processed Img9.jpg\n",
      "Processed Img4.jpg\n",
      "Processed Img1.jpg\n",
      "Processed Img16.jpg\n",
      "Processed Img14.jpg\n",
      "Processed Img8.jpg\n",
      "Processed Img17.jpg\n",
      "Processed Img2.jpg\n",
      "Processed Img5.jpg\n",
      "Processed Img7.jpg\n",
      "Processed Img19.jpg\n",
      "\n",
      "Average Evaluation Metrics:\n",
      "IoU: 0.8048\n",
      "Dice Coefficient: 0.8491\n",
      "Accuracy: 0.9676\n",
      "Precision: 0.8130\n",
      "Recall: 0.8902\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the trained model\n",
    "def load_model(model_path, device):\n",
    "    model = UNet().to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Preprocess the image (same as during training)\n",
    "def preprocess_image(image_path, transform):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    original_size = image.size  # Store original size\n",
    "    image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "    return image, original_size\n",
    "\n",
    "# Postprocess the mask (convert to binary and resize to original)\n",
    "def postprocess_mask(mask_tensor, original_size, threshold=0.5):\n",
    "    mask = mask_tensor.squeeze().cpu().numpy()  # Remove batch dim and convert to numpy\n",
    "    mask = (mask > threshold).astype(np.uint8) * 255  # Threshold and scale to 0-255\n",
    "    mask = Image.fromarray(mask).resize(original_size, Image.NEAREST)\n",
    "    return mask\n",
    "\n",
    "def process_images(model, image_dir, mask_dir, output_dir, transform, device):\n",
    "    \"\"\"\n",
    "    Process images, generate masks, and evaluate against ground truth\n",
    "\n",
    "    Args:\n",
    "        model: Trained U-Net model\n",
    "        image_dir: Directory with input images\n",
    "        mask_dir: Directory with ground truth masks\n",
    "        output_dir: Where to save predictions\n",
    "        transform: Image transformations\n",
    "        device: CUDA/CPU device\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    image_files = [f for f in os.listdir(image_dir) if f.endswith('.jpg') and not f.startswith('.')]\n",
    "\n",
    "    # Initialize metrics\n",
    "    metrics = {\n",
    "        'iou': [],\n",
    "        'dice': [],\n",
    "        'accuracy': [],\n",
    "        'precision': [],\n",
    "        'recall': []\n",
    "    }\n",
    "\n",
    "    for img_file in image_files:\n",
    "        try:\n",
    "            # Load and preprocess image\n",
    "            img_path = os.path.join(image_dir, img_file)\n",
    "            image_tensor, original_size = preprocess_image(img_path, transform)\n",
    "            image_tensor = image_tensor.to(device)\n",
    "\n",
    "            # Generate prediction\n",
    "            with torch.no_grad():\n",
    "                mask_tensor = model(image_tensor)\n",
    "\n",
    "            # Postprocess predicted mask\n",
    "            pred_mask = postprocess_mask(mask_tensor, original_size)\n",
    "            pred_mask_np = np.array(pred_mask)\n",
    "\n",
    "            # Save predicted mask\n",
    "            mask_filename = os.path.splitext(img_file)[0] + '_mask.png'\n",
    "            pred_mask.save(os.path.join(output_dir, mask_filename))\n",
    "\n",
    "            # Load ground truth mask\n",
    "            true_mask_path = os.path.join(mask_dir, img_file)\n",
    "            if os.path.exists(true_mask_path):\n",
    "                true_mask = np.array(Image.open(true_mask_path).convert('L'))\n",
    "\n",
    "                # Calculate metrics\n",
    "                pred_bin = (pred_mask_np > 128).astype(np.uint8)  # Threshold at 128\n",
    "                true_bin = (true_mask > 128).astype(np.uint8)     # Threshold at 128\n",
    "\n",
    "                intersection = np.logical_and(pred_bin, true_bin)\n",
    "                union = np.logical_or(pred_bin, true_bin)\n",
    "\n",
    "                tp = np.sum(intersection)\n",
    "                fp = np.sum(pred_bin) - tp\n",
    "                fn = np.sum(true_bin) - tp\n",
    "                tn = np.sum(np.logical_not(union))\n",
    "\n",
    "                eps = 1e-6  # Avoid division by zero\n",
    "\n",
    "                # Store metrics\n",
    "                metrics['iou'].append(tp / (tp + fp + fn + eps))\n",
    "                metrics['dice'].append((2 * tp) / (2 * tp + fp + fn + eps))\n",
    "                metrics['accuracy'].append((tp + tn) / (tp + tn + fp + fn + eps))\n",
    "                metrics['precision'].append(tp / (tp + fp + eps))\n",
    "                metrics['recall'].append(tp / (tp + fn + eps))\n",
    "\n",
    "                # Visualize with metrics\n",
    "                plt.figure(figsize=(18, 6))\n",
    "\n",
    "                plt.subplot(1, 3, 1)\n",
    "                plt.imshow(Image.open(img_path))\n",
    "                plt.title('Original Image')\n",
    "                plt.axis('off')\n",
    "\n",
    "                plt.subplot(1, 3, 2)\n",
    "                plt.imshow(pred_mask, cmap='gray')\n",
    "                plt.title('Predicted Mask')\n",
    "                plt.axis('off')\n",
    "\n",
    "                plt.subplot(1, 3, 3)\n",
    "                plt.imshow(true_mask, cmap='gray')\n",
    "                plt.title('Ground Truth')\n",
    "                plt.axis('off')\n",
    "\n",
    "                plt.savefig(os.path.join(output_dir, f'vis_{img_file}'), bbox_inches='tight')\n",
    "                plt.close()\n",
    "\n",
    "            print(f\"Processed {img_file}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_file}: {str(e)}\")\n",
    "\n",
    "    # Print average metrics if ground truth was available\n",
    "    if metrics['iou']:\n",
    "        print(\"\\nAverage Evaluation Metrics:\")\n",
    "        print(f\"IoU: {np.mean(metrics['iou']):.4f}\")\n",
    "        print(f\"Dice Coefficient: {np.mean(metrics['dice']):.4f}\")\n",
    "        print(f\"Accuracy: {np.mean(metrics['accuracy']):.4f}\")\n",
    "        print(f\"Precision: {np.mean(metrics['precision']):.4f}\")\n",
    "        print(f\"Recall: {np.mean(metrics['recall']):.4f}\")\n",
    "\n",
    "# Visualization function\n",
    "def visualize_result(image_path, mask, save_path=None):\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(image)\n",
    "    plt.title('Original Image')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(mask, cmap='gray')\n",
    "    plt.title('Predicted Mask')\n",
    "    plt.axis('off')\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches='tight', dpi=300)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Define transforms (same as training)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # Paths\n",
    "    model_path = '/content/Tomato_dataset/unet_tomato.pth'  # Update with your saved model path\n",
    "    test_image_dir = '/content/Tomato_dataset/Test2'  # Directory with new images to test\n",
    "    output_dir = '/content/Tomato_dataset/output_masks'    # Where to save results\n",
    "    mask_dir = '/content/Tomato_dataset/Mask2'\n",
    "    # Load model\n",
    "    model = load_model(model_path, device)\n",
    "\n",
    "    # Process images\n",
    "    process_images(model, test_image_dir, mask_dir, output_dir, transform, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RLh8v8dxWBRb",
    "outputId": "45e0d86f-1201-483d-dbf5-d4749290c879"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: content/Tomato_dataset/ (stored 0%)\n",
      "  adding: content/Tomato_dataset/Mask2/ (stored 0%)\n",
      "  adding: content/Tomato_dataset/Mask2/Img10.jpg (deflated 39%)\n",
      "  adding: content/Tomato_dataset/Mask2/Img6.jpg (deflated 47%)\n",
      "  adding: content/Tomato_dataset/Mask2/Img20.jpg (deflated 52%)\n",
      "  adding: content/Tomato_dataset/Mask2/Img12.jpg (deflated 49%)\n",
      "  adding: content/Tomato_dataset/Mask2/Img11.jpg (deflated 39%)\n",
      "  adding: content/Tomato_dataset/Mask2/Img18.jpg (deflated 40%)\n",
      "  adding: content/Tomato_dataset/Mask2/Img13.jpg (deflated 44%)\n",
      "  adding: content/Tomato_dataset/Mask2/Img3.jpg (deflated 84%)\n",
      "  adding: content/Tomato_dataset/Mask2/Img15.jpg (deflated 41%)\n",
      "  adding: content/Tomato_dataset/Mask2/Img9.jpg (deflated 46%)\n",
      "  adding: content/Tomato_dataset/Mask2/Img4.jpg (deflated 31%)\n",
      "  adding: content/Tomato_dataset/Mask2/Img1.jpg (deflated 44%)\n",
      "  adding: content/Tomato_dataset/Mask2/Img16.jpg (deflated 44%)\n",
      "  adding: content/Tomato_dataset/Mask2/Img14.jpg (deflated 84%)\n",
      "  adding: content/Tomato_dataset/Mask2/Img8.jpg (deflated 43%)\n",
      "  adding: content/Tomato_dataset/Mask2/Img17.jpg (deflated 40%)\n",
      "  adding: content/Tomato_dataset/Mask2/Img2.jpg (deflated 36%)\n",
      "  adding: content/Tomato_dataset/Mask2/Img5.jpg (deflated 41%)\n",
      "  adding: content/Tomato_dataset/Mask2/Img7.jpg (deflated 41%)\n",
      "  adding: content/Tomato_dataset/Mask2/Img19.jpg (deflated 43%)\n",
      "  adding: content/Tomato_dataset/Mask/ (stored 0%)\n",
      "  adding: content/Tomato_dataset/Mask/Img45.jpg (deflated 41%)\n",
      "  adding: content/Tomato_dataset/Mask/Img59.jpg (deflated 40%)\n",
      "  adding: content/Tomato_dataset/Mask/Img10.jpg (deflated 47%)\n",
      "  adding: content/Tomato_dataset/Mask/Img63.jpg (deflated 43%)\n",
      "  adding: content/Tomato_dataset/Mask/Img50.jpg (deflated 32%)\n",
      "  adding: content/Tomato_dataset/Mask/Img27.jpg (deflated 40%)\n",
      "  adding: content/Tomato_dataset/Mask/Img53.jpg (deflated 40%)\n",
      "  adding: content/Tomato_dataset/Mask/Img6.jpg (deflated 45%)\n",
      "  adding: content/Tomato_dataset/Mask/Img20.jpg (deflated 40%)\n",
      "  adding: content/Tomato_dataset/Mask/Img12.jpg (deflated 38%)\n",
      "  adding: content/Tomato_dataset/Mask/Img55.jpg (deflated 32%)\n",
      "  adding: content/Tomato_dataset/Mask/Img11.jpg (deflated 44%)\n",
      "  adding: content/Tomato_dataset/Mask/Img31.jpg (deflated 44%)\n",
      "  adding: content/Tomato_dataset/Mask/Img25.jpg (deflated 43%)\n",
      "  adding: content/Tomato_dataset/Mask/Img67.jpg (deflated 33%)\n",
      "  adding: content/Tomato_dataset/Mask/Img72.jpg (deflated 42%)\n",
      "  adding: content/Tomato_dataset/Mask/Img39.jpg (deflated 40%)\n",
      "  adding: content/Tomato_dataset/Mask/Img73.jpg (deflated 39%)\n",
      "  adding: content/Tomato_dataset/Mask/Img18.jpg (deflated 30%)\n",
      "  adding: content/Tomato_dataset/Mask/Img35.jpg (deflated 45%)\n",
      "  adding: content/Tomato_dataset/Mask/Img70.jpg (deflated 48%)\n",
      "  adding: content/Tomato_dataset/Mask/Img13.jpg (deflated 37%)\n",
      "  adding: content/Tomato_dataset/Mask/Img47.jpg (deflated 84%)\n",
      "  adding: content/Tomato_dataset/Mask/Img54.jpg (deflated 53%)\n",
      "  adding: content/Tomato_dataset/Mask/Img3.jpg (deflated 49%)\n",
      "  adding: content/Tomato_dataset/Mask/Img40.jpg (deflated 36%)\n",
      "  adding: content/Tomato_dataset/Mask/Img64.jpg (deflated 43%)\n",
      "  adding: content/Tomato_dataset/Mask/Img32.jpg (deflated 44%)\n",
      "  adding: content/Tomato_dataset/Mask/Img57.jpg (deflated 38%)\n",
      "  adding: content/Tomato_dataset/Mask/Img15.jpg (deflated 44%)\n",
      "  adding: content/Tomato_dataset/Mask/Img52.jpg (deflated 45%)\n",
      "  adding: content/Tomato_dataset/Mask/Img23.jpg (deflated 40%)\n",
      "  adding: content/Tomato_dataset/Mask/Img24.jpg (deflated 46%)\n",
      "  adding: content/Tomato_dataset/Mask/Img9.jpg (deflated 46%)\n",
      "  adding: content/Tomato_dataset/Mask/Img4.jpg (deflated 46%)\n",
      "  adding: content/Tomato_dataset/Mask/Img28.jpg (deflated 60%)\n",
      "  adding: content/Tomato_dataset/Mask/Img65.jpg (deflated 38%)\n",
      "  adding: content/Tomato_dataset/Mask/Img76.jpg (deflated 37%)\n",
      "  adding: content/Tomato_dataset/Mask/Img80.jpg (deflated 35%)\n",
      "  adding: content/Tomato_dataset/Mask/Img60.jpg (deflated 84%)\n",
      "  adding: content/Tomato_dataset/Mask/Img1.jpg (deflated 37%)\n",
      "  adding: content/Tomato_dataset/Mask/Img51.jpg (deflated 84%)\n",
      "  adding: content/Tomato_dataset/Mask/Img36.jpg (deflated 43%)\n",
      "  adding: content/Tomato_dataset/Mask/Img42.jpg (deflated 45%)\n",
      "  adding: content/Tomato_dataset/Mask/Img16.jpg (deflated 30%)\n",
      "  adding: content/Tomato_dataset/Mask/Img33.jpg (deflated 43%)\n",
      "  adding: content/Tomato_dataset/Mask/Img14.jpg (deflated 37%)\n",
      "  adding: content/Tomato_dataset/Mask/Img21.jpg (deflated 40%)\n",
      "  adding: content/Tomato_dataset/Mask/Img8.jpg (deflated 84%)\n",
      "  adding: content/Tomato_dataset/Mask/Img58.jpg (deflated 43%)\n",
      "  adding: content/Tomato_dataset/Mask/Img66.jpg (deflated 43%)\n",
      "  adding: content/Tomato_dataset/Mask/Img17.jpg (deflated 31%)\n",
      "  adding: content/Tomato_dataset/Mask/Img44.jpg (deflated 40%)\n",
      "  adding: content/Tomato_dataset/Mask/Img61.jpg (deflated 43%)\n",
      "  adding: content/Tomato_dataset/Mask/Img71.jpg (deflated 42%)\n",
      "  adding: content/Tomato_dataset/Mask/Img46.jpg (deflated 47%)\n",
      "  adding: content/Tomato_dataset/Mask/Img56.jpg (deflated 46%)\n",
      "  adding: content/Tomato_dataset/Mask/Img41.jpg (deflated 46%)\n",
      "  adding: content/Tomato_dataset/Mask/Img26.jpg (deflated 49%)\n",
      "  adding: content/Tomato_dataset/Mask/Img69.jpg (deflated 45%)\n",
      "  adding: content/Tomato_dataset/Mask/Img78.jpg (deflated 84%)\n",
      "  adding: content/Tomato_dataset/Mask/Img75.jpg (deflated 37%)\n",
      "  adding: content/Tomato_dataset/Mask/Img2.jpg (deflated 44%)\n",
      "  adding: content/Tomato_dataset/Mask/Img5.jpg (deflated 84%)\n",
      "  adding: content/Tomato_dataset/Mask/Img7.jpg (deflated 50%)\n",
      "  adding: content/Tomato_dataset/Mask/Img30.jpg (deflated 34%)\n",
      "  adding: content/Tomato_dataset/Mask/Img37.jpg (deflated 84%)\n",
      "  adding: content/Tomato_dataset/Mask/Img68.jpg (deflated 40%)\n",
      "  adding: content/Tomato_dataset/Mask/Img29.jpg (deflated 61%)\n",
      "  adding: content/Tomato_dataset/Mask/Img22.jpg (deflated 40%)\n",
      "  adding: content/Tomato_dataset/Mask/Img43.jpg (deflated 42%)\n",
      "  adding: content/Tomato_dataset/Mask/Img79.jpg (deflated 40%)\n",
      "  adding: content/Tomato_dataset/Mask/Img74.jpg (deflated 36%)\n",
      "  adding: content/Tomato_dataset/Mask/Img62.jpg (deflated 49%)\n",
      "  adding: content/Tomato_dataset/Mask/Img38.jpg (deflated 45%)\n",
      "  adding: content/Tomato_dataset/Mask/Img19.jpg (deflated 38%)\n",
      "  adding: content/Tomato_dataset/Mask/Img77.jpg (deflated 38%)\n",
      "  adding: content/Tomato_dataset/Mask/Img34.jpg (deflated 48%)\n",
      "  adding: content/Tomato_dataset/Mask/Img48.jpg (deflated 39%)\n",
      "  adding: content/Tomato_dataset/Mask/Img49.jpg (deflated 41%)\n",
      "  adding: content/Tomato_dataset/Test/ (stored 0%)\n",
      "  adding: content/Tomato_dataset/Test/Img10.jpg (deflated 4%)\n",
      "  adding: content/Tomato_dataset/Test/Img6.jpg (deflated 4%)\n",
      "  adding: content/Tomato_dataset/Test/Img20.jpg (deflated 6%)\n",
      "  adding: content/Tomato_dataset/Test/Img12.jpg (deflated 5%)\n",
      "  adding: content/Tomato_dataset/Test/Img11.jpg (deflated 5%)\n",
      "  adding: content/Tomato_dataset/Test/Img18.jpg (deflated 4%)\n",
      "  adding: content/Tomato_dataset/Test/Img13.jpg (deflated 8%)\n",
      "  adding: content/Tomato_dataset/Test/Img3.jpg (deflated 4%)\n",
      "  adding: content/Tomato_dataset/Test/Img15.jpg (deflated 5%)\n",
      "  adding: content/Tomato_dataset/Test/Img9.jpg (deflated 5%)\n",
      "  adding: content/Tomato_dataset/Test/Img4.jpg (deflated 4%)\n",
      "  adding: content/Tomato_dataset/Test/Img1.jpg (deflated 6%)\n",
      "  adding: content/Tomato_dataset/Test/Img16.jpg (deflated 6%)\n",
      "  adding: content/Tomato_dataset/Test/Img14.jpg (deflated 4%)\n",
      "  adding: content/Tomato_dataset/Test/Img8.jpg (deflated 5%)\n",
      "  adding: content/Tomato_dataset/Test/Img17.jpg (deflated 5%)\n",
      "  adding: content/Tomato_dataset/Test/Img2.jpg (deflated 4%)\n",
      "  adding: content/Tomato_dataset/Test/Img5.jpg (deflated 4%)\n",
      "  adding: content/Tomato_dataset/Test/Img7.jpg (deflated 5%)\n",
      "  adding: content/Tomato_dataset/Test/Img19.jpg (deflated 6%)\n",
      "  adding: content/Tomato_dataset/unet_tomato.pth (deflated 8%)\n",
      "  adding: content/Tomato_dataset/Train2/ (stored 0%)\n",
      "  adding: content/Tomato_dataset/Train2/Img45.jpg (deflated 3%)\n",
      "  adding: content/Tomato_dataset/Train2/Img59.jpg (deflated 3%)\n",
      "  adding: content/Tomato_dataset/Train2/Img10.jpg (deflated 3%)\n",
      "  adding: content/Tomato_dataset/Train2/Img63.jpg (deflated 3%)\n",
      "  adding: content/Tomato_dataset/Train2/Img50.jpg (deflated 4%)\n",
      "  adding: content/Tomato_dataset/Train2/Img27.jpg (deflated 3%)\n",
      "  adding: content/Tomato_dataset/Train2/Img53.jpg (deflated 2%)\n",
      "  adding: content/Tomato_dataset/Train2/Img6.jpg (deflated 4%)\n",
      "  adding: content/Tomato_dataset/Train2/Img20.jpg (deflated 3%)\n",
      "  adding: content/Tomato_dataset/Train2/Img12.jpg (deflated 2%)\n",
      "  adding: content/Tomato_dataset/Train2/Img55.jpg (deflated 2%)\n",
      "  adding: content/Tomato_dataset/Train2/Img11.jpg (deflated 3%)\n",
      "  adding: content/Tomato_dataset/Train2/Img31.jpg (deflated 3%)\n",
      "  adding: content/Tomato_dataset/Train2/Img25.jpg (deflated 3%)\n",
      "  adding: content/Tomato_dataset/Train2/Img67.jpg (deflated 4%)\n",
      "  adding: content/Tomato_dataset/Train2/Img72.jpg (deflated 4%)\n",
      "  adding: content/Tomato_dataset/Train2/Img39.jpg (deflated 3%)\n",
      "  adding: content/Tomato_dataset/Train2/Img73.jpg (deflated 3%)\n",
      "  adding: content/Tomato_dataset/Train2/Img18.jpg (deflated 3%)\n",
      "  adding: content/Tomato_dataset/Train2/Img35.jpg (deflated 3%)\n",
      "  adding: content/Tomato_dataset/Train2/Img70.jpg (deflated 3%)\n",
      "  adding: content/Tomato_dataset/Train2/Img13.jpg (deflated 3%)\n",
      "  adding: content/Tomato_dataset/Train2/Img47.jpg (deflated 3%)\n",
      "  adding: content/Tomato_dataset/Train2/Img54.jpg (deflated 6%)\n",
      "  adding: content/Tomato_dataset/Train2/Img3.jpg (deflated 4%)\n",
      "  adding: content/Tomato_dataset/Train2/Img40.jpg (deflated 3%)\n",
      "  adding: content/Tomato_dataset/Train2/Img64.jpg (deflated 3%)\n",
      "  adding: content/Tomato_dataset/Train2/Img32.jpg (deflated 3%)\n",
      "  adding: content/Tomato_dataset/Train2/Img57.jpg (deflated 3%)\n",
      "  adding: content/Tomato_dataset/Train2/Img15.jpg (deflated 3%)\n",
      "  adding: content/Tomato_dataset/Train2/Img52.jpg (deflated 3%)\n",
      "  adding: content/Tomato_dataset/Train2/Img23.jpg (deflated 4%)\n",
      "  adding: content/Tomato_dataset/Train2/Img24.jpg (deflated 3%)\n",
      "  adding: content/Tomato_dataset/Train2/Img9.jpg (deflated 3%)\n",
      "  adding: content/Tomato_dataset/Train2/Img4.jpg (deflated 3%)\n",
      "  adding: content/Tomato_dataset/Train2/Img28.jpg (deflated 9%)\n",
      "  adding: content/Tomato_dataset/Train2/Img65.jpg (deflated 3%)\n",
      "  adding: content/Tomato_dataset/Train2/Img76.jpg (deflated 3%)\n",
      "  adding: content/Tomato_dataset/Train2/Img80.jpg (deflated 2%)\n",
      "  adding: content/Tomato_dataset/Train2/Img60.jpg (deflated 3%)\n",
      "  adding: content/Tomato_dataset/Train2/Img1.jpg (deflated 3%)\n",
      "  adding: content/Tomato_dataset/Train2/Img51.jpg (deflated 2%)\n",
      "  adding: content/Tomato_dataset/Train2/Img36.jpg (deflated 3%)\n",
      "  adding: content/Tomato_dataset/Train2/Img42.jpg (deflated 4%)\n",
      "  adding: content/Tomato_dataset/Train2/Img16.jpg (deflated 2%)\n",
      "  adding: content/Tomato_dataset/Train2/Img33.jpg (deflated 3%)\n",
      "  adding: content/Tomato_dataset/Train2/Img14.jpg (deflated 2%)\n",
      "  adding: content/Tomato_dataset/Train2/Img21.jpg (deflated 3%)\n",
      "  adding: content/Tomato_dataset/Train2/Img8.jpg (deflated 4%)\n",
      "  adding: content/Tomato_dataset/Train2/Img58.jpg (deflated 3%)\n",
      "  adding: content/Tomato_dataset/Train2/Img66.jpg (deflated 3%)\n",
      "  adding: content/Tomato_dataset/Train2/Img17.jpg (deflated 2%)\n",
      "  adding: content/Tomato_dataset/Train2/Img44.jpg (deflated 3%)\n",
      "  adding: content/Tomato_dataset/Train2/Img61.jpg (deflated 3%)\n",
      "  adding: content/Tomato_dataset/Train2/Img71.jpg (deflated 4%)\n",
      "  adding: content/Tomato_dataset/Train2/Img46.jpg (deflated 3%)\n",
      "  adding: content/Tomato_dataset/Train2/Img56.jpg (deflated 3%)\n",
      "  adding: content/Tomato_dataset/Train2/Img41.jpg (deflated 4%)\n",
      "  adding: content/Tomato_dataset/Train2/Img26.jpg (deflated 4%)\n",
      "  adding: content/Tomato_dataset/Train2/Img69.jpg (deflated 3%)\n",
      "  adding: content/Tomato_dataset/Train2/Img78.jpg (deflated 2%)\n",
      "  adding: content/Tomato_dataset/Train2/Img75.jpg (deflated 3%)\n",
      "  adding: content/Tomato_dataset/Train2/Img2.jpg (deflated 4%)\n",
      "  adding: content/Tomato_dataset/Train2/Img5.jpg (deflated 3%)\n",
      "  adding: content/Tomato_dataset/Train2/Img7.jpg (deflated 4%)\n",
      "  adding: content/Tomato_dataset/Train2/Img30.jpg (deflated 3%)\n",
      "  adding: content/Tomato_dataset/Train2/Img37.jpg (deflated 3%)\n",
      "  adding: content/Tomato_dataset/Train2/Img68.jpg (deflated 3%)\n",
      "  adding: content/Tomato_dataset/Train2/Img29.jpg (deflated 10%)\n",
      "  adding: content/Tomato_dataset/Train2/Img22.jpg (deflated 3%)\n",
      "  adding: content/Tomato_dataset/Train2/Img43.jpg (deflated 4%)\n",
      "  adding: content/Tomato_dataset/Train2/Img79.jpg (deflated 3%)\n",
      "  adding: content/Tomato_dataset/Train2/Img74.jpg (deflated 2%)\n",
      "  adding: content/Tomato_dataset/Train2/Img62.jpg (deflated 4%)\n",
      "  adding: content/Tomato_dataset/Train2/Img38.jpg (deflated 3%)\n",
      "  adding: content/Tomato_dataset/Train2/Img19.jpg (deflated 3%)\n",
      "  adding: content/Tomato_dataset/Train2/Img77.jpg (deflated 3%)\n",
      "  adding: content/Tomato_dataset/Train2/Img34.jpg (deflated 3%)\n",
      "  adding: content/Tomato_dataset/Train2/Img48.jpg (deflated 3%)\n",
      "  adding: content/Tomato_dataset/Train2/Img49.jpg (deflated 3%)\n",
      "  adding: content/Tomato_dataset/Test2/ (stored 0%)\n",
      "  adding: content/Tomato_dataset/Test2/Img10.jpg (deflated 2%)\n",
      "  adding: content/Tomato_dataset/Test2/Img6.jpg (deflated 3%)\n",
      "  adding: content/Tomato_dataset/Test2/Img20.jpg (deflated 3%)\n",
      "  adding: content/Tomato_dataset/Test2/Img12.jpg (deflated 3%)\n",
      "  adding: content/Tomato_dataset/Test2/Img11.jpg (deflated 3%)\n",
      "  adding: content/Tomato_dataset/Test2/Img18.jpg (deflated 2%)\n",
      "  adding: content/Tomato_dataset/Test2/Img13.jpg (deflated 4%)\n",
      "  adding: content/Tomato_dataset/Test2/Img3.jpg (deflated 2%)\n",
      "  adding: content/Tomato_dataset/Test2/Img15.jpg (deflated 3%)\n",
      "  adding: content/Tomato_dataset/Test2/Img9.jpg (deflated 3%)\n",
      "  adding: content/Tomato_dataset/Test2/Img4.jpg (deflated 3%)\n",
      "  adding: content/Tomato_dataset/Test2/Img1.jpg (deflated 3%)\n",
      "  adding: content/Tomato_dataset/Test2/Img16.jpg (deflated 3%)\n",
      "  adding: content/Tomato_dataset/Test2/Img14.jpg (deflated 3%)\n",
      "  adding: content/Tomato_dataset/Test2/Img8.jpg (deflated 3%)\n",
      "  adding: content/Tomato_dataset/Test2/Img17.jpg (deflated 3%)\n",
      "  adding: content/Tomato_dataset/Test2/Img2.jpg (deflated 2%)\n",
      "  adding: content/Tomato_dataset/Test2/Img5.jpg (deflated 3%)\n",
      "  adding: content/Tomato_dataset/Test2/Img7.jpg (deflated 3%)\n",
      "  adding: content/Tomato_dataset/Test2/Img19.jpg (deflated 3%)\n",
      "  adding: content/Tomato_dataset/Train/ (stored 0%)\n",
      "  adding: content/Tomato_dataset/Train/Img45.jpg (deflated 5%)\n",
      "  adding: content/Tomato_dataset/Train/Img59.jpg (deflated 7%)\n",
      "  adding: content/Tomato_dataset/Train/Img10.jpg (deflated 6%)\n",
      "  adding: content/Tomato_dataset/Train/Img63.jpg (deflated 6%)\n",
      "  adding: content/Tomato_dataset/Train/Img50.jpg (deflated 5%)\n",
      "  adding: content/Tomato_dataset/Train/Img27.jpg (deflated 5%)\n",
      "  adding: content/Tomato_dataset/Train/Img53.jpg (deflated 5%)\n",
      "  adding: content/Tomato_dataset/Train/Img6.jpg (deflated 7%)\n",
      "  adding: content/Tomato_dataset/Train/Img20.jpg (deflated 5%)\n",
      "  adding: content/Tomato_dataset/Train/Img12.jpg (deflated 3%)\n",
      "  adding: content/Tomato_dataset/Train/Img55.jpg (deflated 6%)\n",
      "  adding: content/Tomato_dataset/Train/Img11.jpg (deflated 5%)\n",
      "  adding: content/Tomato_dataset/Train/Img31.jpg (deflated 5%)\n",
      "  adding: content/Tomato_dataset/Train/Img25.jpg (deflated 6%)\n",
      "  adding: content/Tomato_dataset/Train/Img67.jpg (deflated 10%)\n",
      "  adding: content/Tomato_dataset/Train/Img72.jpg (deflated 8%)\n",
      "  adding: content/Tomato_dataset/Train/Img39.jpg (deflated 4%)\n",
      "  adding: content/Tomato_dataset/Train/Img73.jpg (deflated 4%)\n",
      "  adding: content/Tomato_dataset/Train/Img18.jpg (deflated 4%)\n",
      "  adding: content/Tomato_dataset/Train/Img35.jpg (deflated 7%)\n",
      "  adding: content/Tomato_dataset/Train/Img70.jpg (deflated 5%)\n",
      "  adding: content/Tomato_dataset/Train/Img13.jpg (deflated 5%)\n",
      "  adding: content/Tomato_dataset/Train/Img47.jpg (deflated 7%)\n",
      "  adding: content/Tomato_dataset/Train/Img54.jpg (deflated 13%)\n",
      "  adding: content/Tomato_dataset/Train/Img3.jpg (deflated 7%)\n",
      "  adding: content/Tomato_dataset/Train/Img40.jpg (deflated 5%)\n",
      "  adding: content/Tomato_dataset/Train/Img64.jpg (deflated 6%)\n",
      "  adding: content/Tomato_dataset/Train/Img32.jpg (deflated 5%)\n",
      "  adding: content/Tomato_dataset/Train/Img57.jpg (deflated 7%)\n",
      "  adding: content/Tomato_dataset/Train/Img15.jpg (deflated 6%)\n",
      "  adding: content/Tomato_dataset/Train/Img52.jpg (deflated 4%)\n",
      "  adding: content/Tomato_dataset/Train/Img23.jpg (deflated 8%)\n",
      "  adding: content/Tomato_dataset/Train/Img24.jpg (deflated 5%)\n",
      "  adding: content/Tomato_dataset/Train/Img9.jpg (deflated 5%)\n",
      "  adding: content/Tomato_dataset/Train/Img4.jpg (deflated 5%)\n",
      "  adding: content/Tomato_dataset/Train/Img28.jpg (deflated 14%)\n",
      "  adding: content/Tomato_dataset/Train/Img65.jpg (deflated 5%)\n",
      "  adding: content/Tomato_dataset/Train/Img76.jpg (deflated 5%)\n",
      "  adding: content/Tomato_dataset/Train/Img80.jpg (deflated 4%)\n",
      "  adding: content/Tomato_dataset/Train/Img60.jpg (deflated 7%)\n",
      "  adding: content/Tomato_dataset/Train/Img1.jpg (deflated 7%)\n",
      "  adding: content/Tomato_dataset/Train/Img51.jpg (deflated 4%)\n",
      "  adding: content/Tomato_dataset/Train/Img36.jpg (deflated 4%)\n",
      "  adding: content/Tomato_dataset/Train/Img42.jpg (deflated 9%)\n",
      "  adding: content/Tomato_dataset/Train/Img16.jpg (deflated 3%)\n",
      "  adding: content/Tomato_dataset/Train/Img33.jpg (deflated 5%)\n",
      "  adding: content/Tomato_dataset/Train/Img14.jpg (deflated 4%)\n",
      "  adding: content/Tomato_dataset/Train/Img21.jpg (deflated 5%)\n",
      "  adding: content/Tomato_dataset/Train/Img8.jpg (deflated 8%)\n",
      "  adding: content/Tomato_dataset/Train/Img58.jpg (deflated 7%)\n",
      "  adding: content/Tomato_dataset/Train/Img66.jpg (deflated 8%)\n",
      "  adding: content/Tomato_dataset/Train/Img17.jpg (deflated 3%)\n",
      "  adding: content/Tomato_dataset/Train/Img44.jpg (deflated 6%)\n",
      "  adding: content/Tomato_dataset/Train/Img61.jpg (deflated 6%)\n",
      "  adding: content/Tomato_dataset/Train/Img71.jpg (deflated 9%)\n",
      "  adding: content/Tomato_dataset/Train/Img46.jpg (deflated 7%)\n",
      "  adding: content/Tomato_dataset/Train/Img56.jpg (deflated 6%)\n",
      "  adding: content/Tomato_dataset/Train/Img41.jpg (deflated 9%)\n",
      "  adding: content/Tomato_dataset/Train/Img26.jpg (deflated 9%)\n",
      "  adding: content/Tomato_dataset/Train/Img69.jpg (deflated 8%)\n",
      "  adding: content/Tomato_dataset/Train/Img78.jpg (deflated 3%)\n",
      "  adding: content/Tomato_dataset/Train/Img75.jpg (deflated 5%)\n",
      "  adding: content/Tomato_dataset/Train/Img2.jpg (deflated 8%)\n",
      "  adding: content/Tomato_dataset/Train/Img5.jpg (deflated 6%)\n",
      "  adding: content/Tomato_dataset/Train/Img7.jpg (deflated 9%)\n",
      "  adding: content/Tomato_dataset/Train/Img30.jpg (deflated 5%)\n",
      "  adding: content/Tomato_dataset/Train/Img37.jpg (deflated 6%)\n",
      "  adding: content/Tomato_dataset/Train/Img68.jpg (deflated 6%)\n",
      "  adding: content/Tomato_dataset/Train/Img29.jpg (deflated 15%)\n",
      "  adding: content/Tomato_dataset/Train/Img22.jpg (deflated 6%)\n",
      "  adding: content/Tomato_dataset/Train/Img43.jpg (deflated 9%)\n",
      "  adding: content/Tomato_dataset/Train/Img79.jpg (deflated 6%)\n",
      "  adding: content/Tomato_dataset/Train/Img74.jpg (deflated 4%)\n",
      "  adding: content/Tomato_dataset/Train/Img62.jpg (deflated 9%)\n",
      "  adding: content/Tomato_dataset/Train/Img38.jpg (deflated 8%)\n",
      "  adding: content/Tomato_dataset/Train/Img19.jpg (deflated 5%)\n",
      "  adding: content/Tomato_dataset/Train/Img77.jpg (deflated 4%)\n",
      "  adding: content/Tomato_dataset/Train/Img34.jpg (deflated 7%)\n",
      "  adding: content/Tomato_dataset/Train/Img48.jpg (deflated 5%)\n",
      "  adding: content/Tomato_dataset/Train/Img49.jpg (deflated 8%)\n",
      "  adding: content/Tomato_dataset/output_masks/ (stored 0%)\n",
      "  adding: content/Tomato_dataset/output_masks/vis_Img18.jpg (deflated 33%)\n",
      "  adding: content/Tomato_dataset/output_masks/vis_Img15.jpg (deflated 32%)\n",
      "  adding: content/Tomato_dataset/output_masks/vis_Img5.jpg (deflated 33%)\n",
      "  adding: content/Tomato_dataset/output_masks/vis_Img6.jpg (deflated 34%)\n",
      "  adding: content/Tomato_dataset/output_masks/vis_Img7.jpg (deflated 34%)\n",
      "  adding: content/Tomato_dataset/output_masks/vis_Img14.jpg (deflated 37%)\n",
      "  adding: content/Tomato_dataset/output_masks/Img14_mask.png (deflated 12%)\n",
      "  adding: content/Tomato_dataset/output_masks/vis_Img8.jpg (deflated 34%)\n",
      "  adding: content/Tomato_dataset/output_masks/Img3_mask.png (deflated 4%)\n",
      "  adding: content/Tomato_dataset/output_masks/vis_Img1.jpg (deflated 35%)\n",
      "  adding: content/Tomato_dataset/output_masks/Img10_mask.png (deflated 7%)\n",
      "  adding: content/Tomato_dataset/output_masks/Img12_mask.png (deflated 13%)\n",
      "  adding: content/Tomato_dataset/output_masks/Img18_mask.png (deflated 3%)\n",
      "  adding: content/Tomato_dataset/output_masks/vis_Img16.jpg (deflated 34%)\n",
      "  adding: content/Tomato_dataset/output_masks/Img2_mask.png (deflated 3%)\n",
      "  adding: content/Tomato_dataset/output_masks/Img1_mask.png (deflated 5%)\n",
      "  adding: content/Tomato_dataset/output_masks/vis_Img10.jpg (deflated 32%)\n",
      "  adding: content/Tomato_dataset/output_masks/vis_Img20.jpg (deflated 37%)\n",
      "  adding: content/Tomato_dataset/output_masks/vis_Img17.jpg (deflated 34%)\n",
      "  adding: content/Tomato_dataset/output_masks/vis_Img13.jpg (deflated 35%)\n",
      "  adding: content/Tomato_dataset/output_masks/vis_Img11.jpg (deflated 33%)\n",
      "  adding: content/Tomato_dataset/output_masks/Img15_mask.png (deflated 4%)\n",
      "  adding: content/Tomato_dataset/output_masks/vis_Img2.jpg (deflated 29%)\n",
      "  adding: content/Tomato_dataset/output_masks/Img9_mask.png (deflated 8%)\n",
      "  adding: content/Tomato_dataset/output_masks/Img8_mask.png (deflated 5%)\n",
      "  adding: content/Tomato_dataset/output_masks/Img17_mask.png (deflated 5%)\n",
      "  adding: content/Tomato_dataset/output_masks/Img19_mask.png (deflated 7%)\n",
      "  adding: content/Tomato_dataset/output_masks/Img6_mask.png (deflated 5%)\n",
      "  adding: content/Tomato_dataset/output_masks/Img13_mask.png (deflated 11%)\n",
      "  adding: content/Tomato_dataset/output_masks/vis_Img4.jpg (deflated 31%)\n",
      "  adding: content/Tomato_dataset/output_masks/Img11_mask.png (deflated 4%)\n",
      "  adding: content/Tomato_dataset/output_masks/Img20_mask.png (deflated 11%)\n",
      "  adding: content/Tomato_dataset/output_masks/Img4_mask.png (deflated 6%)\n",
      "  adding: content/Tomato_dataset/output_masks/vis_Img19.jpg (deflated 35%)\n",
      "  adding: content/Tomato_dataset/output_masks/vis_Img12.jpg (deflated 36%)\n",
      "  adding: content/Tomato_dataset/output_masks/vis_Img9.jpg (deflated 35%)\n",
      "  adding: content/Tomato_dataset/output_masks/Img5_mask.png (deflated 6%)\n",
      "  adding: content/Tomato_dataset/output_masks/Img7_mask.png (deflated 7%)\n",
      "  adding: content/Tomato_dataset/output_masks/Img16_mask.png (deflated 9%)\n",
      "  adding: content/Tomato_dataset/output_masks/vis_Img3.jpg (deflated 36%)\n"
     ]
    }
   ],
   "source": [
    "# Zip the dataset folder (replace 'tomato_dataset' with your folder name)\n",
    "!zip -r tomato_dataset.zip /content/Tomato_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "CHJ9lc0PZJHs",
    "outputId": "720e5943-94ca-4ee0-f6be-c2ae2de0640f"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_56e26fe3-ae41-4350-b828-0fbc9a74fcf7\", \"tomato_dataset.zip\", 118183191)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from google.colab import files\n",
    "files.download('tomato_dataset.zip')  # Download outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "HqCGs3GNZPEh"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
